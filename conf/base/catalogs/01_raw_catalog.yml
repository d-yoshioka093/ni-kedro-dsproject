# Here you can define all your data sets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://kedro.readthedocs.io/en/stable/data/data_catalog.html
#
# We support interacting with a variety of data stores including local file systems, cloud, network and HDFS
#
# The Data Catalog supports being able to reference the same file using two different DataSet implementations
# (transcoding), templating and a way to reuse arguments that are frequently repeated. See more here:
# https://kedro.readthedocs.io/en/stable/data/data_catalog.html
#
# raw_boat_data:
#     type: "${datasets.spark}"  # nested paths into global dict are allowed
#     filepath: "s3a://${bucket_name}/${key_prefix}/${folders.raw}/boats.csv"
#     file_format: parquet

# raw_car_data:
#     type: "${datasets.csv}"
#     filepath: "s3://${bucket_name}/data/${key_prefix}/${folders.raw}/${filename|cars.csv}"  # default to 'cars.csv' if the 'filename' key is not found in the global dict

_csv: &csv
  type: spark.SparkDataSet
  file_format: csv
  load_args:
    sep: ','
    header: True
    inferSchema: False

# train:
#     filepath: data/01_raw/train.csv
#     type: pandas.CSVDataSet
#     load_args:
#         header: "infer"
#     save_args:
#         header: true
#         mode: overwrite
#     # <<: *csv

train:
  type: spark.SparkDataSet
  # filepath: data/01_raw/train.csv
  filepath: ${data_path}/${folders.raw}/train.csv
  file_format: csv
  load_args:
    header: True
    inferSchema: True
  save_args:
    sep: ','
    header: True

result:
  type: spark.SparkDataSet
  # filepath: data/02_intermediate/result.parquet
  filepath: ${data_path}/${folders.int}/result.parquet
  save_args:
    mode: overwrite

result_int:
  type: spark.SparkDataSet
  # filepath: data/02_intermediate/result.parquet
  filepath: ${data_path}/${folders.int}/result_int.parquet
  save_args:
    mode: overwrite
# result:
#     <<: *csv
#     filepath: data\01_raw\result.csv

# result:
#   # type: pickle.PickleDataSet
#   # filepath: data/01_raw/train.pkl
#   # backend: pickle
#     type: kedro.extras.datasets.spark.SparkDataSet
#     filepath: data/01_raw/result.csv
#     file_format: csv
#     load_args:
#         header: true
#         inferSchema: true
#     save_args:
#         header: true
#         mode: overwrite